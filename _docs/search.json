[
  {
    "objectID": "why_good_init.html",
    "href": "why_good_init.html",
    "title": "Why does the initialization matter?",
    "section": "",
    "text": "import torch\n\nIf we take a vector x of shape (1x512) and a matrix a of shape (512x512), initialize them randomly, and multiply it by a pseudo 100 layer network (aka multiply them 100 times), what happens?\n\nx = torch.randn(512)\na = torch.randn(512,512)\n\nfor i in range(100):\n    x = a @ x\n\nx.mean(), x.std()\n\n(tensor(nan), tensor(nan))\n\n\nThis phenominon is called activation explosion, where the activations go to NaN. We can figure out exactly when that happens:\n\nCodeCode + Explanation\n\n\n\nx = torch.randn(512)\na = torch.randn(512,512)\n\nfor i in range(100):\n    x = a @ x\n    if x.std() != x.std():\n        break\n\n\n\n\nx = torch.randn(512)\na = torch.randn(512,512)\n\nfor i in range(100):\n    x = a @ x\n    if x.std() != x.std():\n        break\n\n\nif x.std() != x.std()\n\nNaN numbers will always return False against itself\n\n\n\n\n\ni\n\n27\n\n\nIt took 26 multiplication before the gradients died and the activations could no longer be kept track of.\nWhat happens instead if we make a extremely small and have the activations scale slowly?\n\nx = torch.randn(512)\na = torch.randn(512,512) * 0.01\nfor i in range(100):\n    x = a @ x\n    \nx.mean(), x.std()\n\n(tensor(0.), tensor(0.))\n\n\nAll of our activations vanished this time. The initialization matters immensely to get a decent starting point.\nThis is also why for decades you couldn’t train deep nn’s.\nNow go back to the other notebook to “To Twitter We Go”",
    "crumbs": [
      "Why does the initialization matter?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fastai 2019 pt2 notes",
    "section": "",
    "text": "This is my “speedrunning” attempt to see how far I can get in Deep Learning from the Foundations before the new course begins.\nThis repo contains annotated notes – in my own special format – that are then hosted live on the docs where they make much more sense!\nThis special format for right now is propritary, but you can easily grasp that it utilizes nbdev to achieve what I was trying to do. If you want a challenge, try to recreate what I made with nbdev v2 (something this isn’t) and force me to reveal my secrets ;) Otherwise they shall remain a (mild) secret until I’ve ported them myself and I feel the time is right.\nBut, tl;dr:\n\nThis is a 1:1 followthrough of the videos in Deep Learning from the Foundations, taken in October of 2022.\nAs a result any issues with the course will have been explored and rectified in these notes\nThese notes are best viewed live directly on https://muellerzr.github.io/fastai-2019-pt2-notes, though the notebooks can still be useful!"
  },
  {
    "objectID": "minibatch_training.html",
    "href": "minibatch_training.html",
    "title": "Impractical Deep Learning for Coders Lesson 2, Minibatch Training",
    "section": "",
    "text": "from coursenotes.nb_03 import get_data\n\nimport torch.nn.functional as F\nimport matplotlib as mpl\n\nfrom torch import nn\nimport math",
    "crumbs": [
      "Impractical Deep Learning for Coders Lesson 2, Minibatch Training"
    ]
  },
  {
    "objectID": "minibatch_training.html#initial-setup",
    "href": "minibatch_training.html#initial-setup",
    "title": "Impractical Deep Learning for Coders Lesson 2, Minibatch Training",
    "section": "Initial Setup",
    "text": "Initial Setup\n\nSetup Data\nThis is based on the previous notebooks setup, explainations should be looked at there\n\nmpl.rcParams['image.cmap'] = 'gray'\n\nx_train,y_train,x_valid,y_valid = get_data()\n\nn,m = x_train.shape\nc = y_train.max()+1\nnh = 50\n\n\nclass BasicModel(nn.Module):\n    \"Basic fully connected model\"\n    def __init__(self, n_in, num_hidden, n_out):\n        super().__init__()\n        self.layers = [\n            nn.Linear(n_in,num_hidden),\n            nn.ReLU(),\n            nn.Linear(num_hidden, n_out)\n        ]\n        \n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nmodel = BasicModel(m, nh, 10)\n\n\npred = model(x_train)",
    "crumbs": [
      "Impractical Deep Learning for Coders Lesson 2, Minibatch Training"
    ]
  },
  {
    "objectID": "minibatch_training.html#a-loss-function-cross-entropy-loss",
    "href": "minibatch_training.html#a-loss-function-cross-entropy-loss",
    "title": "Impractical Deep Learning for Coders Lesson 2, Minibatch Training",
    "section": "A loss function: Cross Entropy Loss",
    "text": "A loss function: Cross Entropy Loss\n\nCodeCode + Explanation\n\n\n\ndef log_softmax(x):\n    return (x.exp() / \n            (x.exp().sum(-1, keepdim=True))\n           ).log()\n\n\n\n\ndef log_softmax(x):\n    return (x.exp() / \n            (x.exp().sum(-1, keepdim=True))\n           ).log()\n\n\ndef log_softmax(x):\n\nLog softmax is simply taking the exponential of x, dividing it by the sum of all the exponentials, and then taking the log of that result\n\n\n           ).log()\n\nWe take the log because negative log likelihood expects a log, not a negative\n\n\n\n\n\nlog_preds = log_softmax(pred)\n\nWe can then calculate log likelihood, which is equal to:\n(classARight * log10(classAProb)) + (classBRight * log10(classBProb))...\nFor example, assume two classes such as above, with the probabilities being 0.98 and 0.2 respectively. The right answer is 0\n\nis_cat = 1 # One hot encoded label\nis_dog = 0 # OHE label\npreds = 0.98 # Softmaxed predictions\nlog_pred_cat = math.log10(preds) # Take log base 10\nlog_pred_dog = math.log10(1-preds) # Take log base 10\n\nnll = -((is_cat * log_pred_cat) + (is_dog * log_pred_dog)); nll # Follow the above, and make it negative\n\n0.00877392430750515\n\n\nWe can make it faster by first finding the location of the 1 (since there is only a single one), then using that index calculate it all\n\ny_train[:3]\n\ntensor([5, 0, 4])\n\n\n\nlog_preds[[0,1,2], [5,0,4]]\n\ntensor([-2.4597, -2.3251, -2.1119], grad_fn=&lt;IndexBackward0&gt;)",
    "crumbs": [
      "Impractical Deep Learning for Coders Lesson 2, Minibatch Training"
    ]
  },
  {
    "objectID": "matmul.html",
    "href": "matmul.html",
    "title": "Lesson 1, Matrix Multiplication (part 1)",
    "section": "",
    "text": "The game: Recreate fastai, while only being able to use:\nThe game I will also be playing:\n3 steps to training a really good model:\nHow to avoid overfitting from A -&gt; F\n4 & 5 both have the least impact, start with the first 3\nFirst we need to download the dataset we are using, which will be MNIST\nwith gzip.open(path, 'rb') as f:\n    ((x_train,y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\nThe downloaded data contains numpy arrays, which are not allowed so they must be converted to tensors",
    "crumbs": [
      "Lesson 1, Matrix Multiplication (part 1)"
    ]
  },
  {
    "objectID": "matmul.html#initial-python-model",
    "href": "matmul.html#initial-python-model",
    "title": "Lesson 1, Matrix Multiplication (part 1)",
    "section": "Initial python model",
    "text": "Initial python model\nCreate a simple linear model, of something akin to y=ax+b\n\nCode\n\nweights = torch.randn(784,10)\nbias = torch.zeros(10)\n\n\n\nExplanation\n\n{\n    \"torch.randn(784,10)\": \"This operates as `a`, a 784x10 matrix where 784==length of the array, 10==num going out\",\n    \"torch.zeros(10)\": \"The bias will just start as 10 zeros\"\n};\n\n\n\nMatrix Multiplication\nCore of the basic of machine learning, “affine functions”.\nCool website to visualize\n\nCode\n\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar,bc)\n    for i in range(ar):\n        for j in range(bc):\n            for k in range(ac): # or br\n                c[i,j] += a[i,k] * b[k,j]\n    return c\n\n\n\nExplanation\n\n{\n    \"(a,b)\": \"`a` and `b` are two matricies which should be multiplied\",\n    \"assert ac==br\": \"Matrix multiplication cannot occur unless the number of columns in `a` aligns with the number of rows in `b`\",\n    \"c\":\"`c` is the resulting matrix, which has a shape of `a`'s rows and `b`'s columns\",\n    \"for i in range(ar):\": \"Loop of matrix B as a whole scrolling down matrix A *sideways*, imagine going row by row like a curtain coming down slowly\",\n    \"for j in range(bc):\": \"Loop of each column in matrix B at each row in matrix A\",\n    \"for k in range(ac):\": \"The actual loop of multiplying and adding (matrix multiplication)\",\n    \"c[i,j] += a[i,k] * b[k,j]\": \"The actual multiplication being performed\"\n};\n\n\n\nCode\n\nm1 = x_valid[:5]\nm2 = weights\n\n\n\nExplanation\n\n{\n    \"m1\": \"Five rows of the validation set\",\n    \"m2\": \"Weight matrix\"\n};\n\n\nm1.shape, m2.shape\n\n(torch.Size([5, 784]), torch.Size([784, 10]))\n\n\n\n\n\nCPU times: user 440 ms, sys: 72.4 ms, total: 513 ms\nWall time: 421 ms\n\n\n\nt1.shape # 5 row, 10 col output\n\ntorch.Size([5, 10])\n\n\n\nlen(x_train)\n\n50000\n\n\nThis is quite slow. To do a single epoch it would take ~20,000 seconds on the computer I’m using to take notes. (50,000 on Jeremy’s).\nThis is also why we don’t write things in Python. It’s unreasonably slow.\nNew goal, can we speed this up 50,000 times\n\n\n\nElementwise operations\nTo speed things up, start with the innermost loop and make things just a little bit faster\n\nThe way to make Python faster is to remove python - Jeremy Howard\n\nEWO’s include (+,-,*,/,&gt;,&lt;,==)\nExample with two tensors:\n\na = tensor([10., 6, -4])\nb = tensor([2., 8, 7])\na,b\n\n(tensor([10.,  6., -4.]), tensor([2., 8., 7.]))\n\n\n\na+b\n\ntensor([12., 14.,  3.])\n\n\nWe performed c[0] = a[0]+b[0], c[1] = a[1] + b[1], …\n\nCode\n\nc = (a &lt; b)\nc = c.float().mean()\n\n\n\nExplanation\n\n{\n    \"c = (a &lt; b)\": \"We performed `c[0] = a[0]&lt; b[0]`, `c[1] = a[1] &lt; b[1]`, ...\",\n    \"c\":\"This becomes a boolean array of `[False, True, True]`, having an average of 2/3's\"\n};\n\n\nc\n\ntensor(0.6667)\n\n\nAlso known as what percentage of a is less than b. We could also perform the same on a rank 2 tensor (a tensor that has 2 dimensions), aka a matrix!\n\nm = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nNote: We only convert the first number to a float as PyTorch will realize this and cast the rest as a float\n\nFrobenius norm:\nI have no idea what this is/remember what this is\n\\[\\|A\\|_F=\\left(\\sum_{i, j=1}^n\\left|a_{i j}\\right|^2\\right)^{1 / 2}\\]\n\n\nCode\n\nn = torch.clone(m) # For readability\n(m*n).sum().sqrt()\n\ntensor(16.8819)\n\n\n\n\nExplanation\n\n{\n    \"i\": \"$$\\\\text{This is the first for loop, and goes from 1 }\\\\rightarrow\\\\text{ n}$$\",\n    \"j\": \"$$\\\\text{This is the second for loop, and goes from 1 }\\\\rightarrow\\\\text{ n as well}$$\",\n    \"m*n\":\"$$\\\\text{This correlates to }\\left|a_{i j}\\\\right|$$\",\n    \".sum()\": \"$$\\\\text{This aligns with }\\sum_{i, j=1}^n\\\\text{, which is equivalent to a product combination of }\\sum_{i \\mathop =1}^m and \\sum_{j \\mathop =1}^n$$\",\n    \".sqrt()\": \"$$\\\\text{This correlates to the 1/2 power, simplified as 'result' }\\left(\\\\text{result}\\\\right)^{1 / 2}$$\"\n};\n\n\n# Editors note: If you have \\r in the latex, use \\\\r\n(m*n).sum().sqrt()\n\ntensor(16.8819)\n\n\n\n\n\nElementwise Matrix Multiplication (rendition 2!)\n\nCode\n\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar,bc)\n    for i in range(ar):\n        for j in range(bc):\n            # Any trailing \",:\" can be removed\n            c[i,j] = (a[i,:] * b[:,j]).sum()\n    return c\n\n\n\nExplanation\n\n{\n    \"c[i,j] = (a[i,:] * b[:,j]).sum()\": \"$$\\\\text{We replace the entire innermost for loop with this, and directly perform the matrix operation.}\\\\newline\\\\text{Remember that : selects everything from i}\\\\rightarrow\\\\text{end! (Or the entirety of that axis)}$$\",\n    \"a[i,:]\": \"We select all of row `i`\",\n    \"b[:,j]\": \"And we select all of column `j`.\"\n};\n\nIn numpy and PyTorch it goes 🎵 row by column 🎵\n\n\n\n570 µs ± 21.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n445/.727\n\n612.1045392022008\n\n\nWe are now 600x faster by removing a single loop by running it in c\n\ntest_close(t1, matmul(m1,m2), eps=1e-4)\n\n\n\n\nBroadcasting\nNow we need to get rid of the second-most inner loop through broadcasting.\nGet rid of all for loops and replace with implicit broadcasted loops\n\nBroadcasting with a scalar\n\na\n\ntensor([10.,  6., -4.])\n\n\n\na &gt; 0\n\ntensor([ True,  True, False])\n\n\nWe just broadcast a &gt; 0. Also known as, the float turns into [0,0,0] and an element-wise operation is performed, and is done at either C or CUDA speed depending on the device\n\n\nBroadcast a vector to a matrix\n\nd = tensor([10.,20,30]); d\n\ntensor([10., 20., 30.])\n\n\n\nm\n\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n\n\n\nm.shape,c.shape\n\n(torch.Size([3, 3]), torch.Size([]))\n\n\n\nm + d\n\ntensor([[11., 22., 33.],\n        [14., 25., 36.],\n        [17., 28., 39.]])\n\n\nBy the rules we have so far, we’d expect this to not actually do anything. But instead it broadcast the tensor horizontally row by row adding the vector to the matrix\n\n\nCode\n\nt = d.expand_as(m)\n\n\n\nExplanation\n\n{\n    \".expand_as\":\"Shows what a tensor (d) would look like if it were broadcast to m\"\n};\n\n\nt\n\ntensor([[10., 20., 30.],\n        [10., 20., 30.],\n        [10., 20., 30.]])\n\n\n\nt.storage()\n\n 10.0\n 20.0\n 30.0\n[torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 3]\n\n\nThis shows us that it’s only storing one copy of t, and not a 3x3 copy of t\n\nt.stride(), t.shape\n\n((0, 1), torch.Size([3, 3]))\n\n\nHow to read this:\nWhen going row by row, it should take zero steps through the memory/storage. And when going column by column, it should take one step.\nThis in turn is how it repeats 10,20,30 for every single row.\nWe can create tensors that behave like tensors much bigger than what they are.\nWhat if we wanted to take a column instead of a row? In other words, a rank 2 tensor of shape (3,1)\n\n\nCode\n\nd.unsqueeze(1)\n\ntensor([[10.],\n        [20.],\n        [30.]])\n\n\n\n\nExplanation\n\n{\n    \".unsqueeze(1)\":\"Adds an additional dimension of 1 to wherever we ask for one\"\n};\n\n\nd.shape\n\ntorch.Size([3])\n\n\n\nd.unsqueeze(0)\n\ntensor([[10., 20., 30.]])\n\n\nd would have a shape of (1,3) which changed from just (3) by adding a dimension at position 0.\n\nd.unsqueeze(1)\n\ntensor([[10.],\n        [20.],\n        [30.]])\n\n\nd would have a shape of (3,1) which changed from just (3) by adding a dimension at position 1.\n\n#d.unsqueeze(2)\n\nIndexError: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n\n\nThis fails because we only have a 1d tensor not a 2d tensor. E.g.:\n\ntorch.tensor([[1,2,3],[4,5,6]]).unsqueeze(2)\n\ntensor([[[1],\n         [2],\n         [3]],\n\n        [[4],\n         [5],\n         [6]]])\n\n\n\nd.shape, d.unsqueeze(0).shape, d.unsqueeze(1).shape\n\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\n\n\n\n\nCode\n\nd.shape, d[None,:].shape, d[:,None].shape\n\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\n\n\n\n\nExplanation\n\n{\n    \"d[None,:], d[:,None]\": \"PyTorch and numpy will use this notation to squeeze in a dimension at index `None`, equivalent to unsqueeze()\",\n    \"d[None,:]\": \"This is equivalent to `d.unsqueeze(0)`\",\n    \"d[:,None]\": \"This is equivalent to `d.unsqueeze(1)`\"\n};\n\n\nd.shape, d[None,:].shape, d[:,None].shape\n\n(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))\n\n\nThis also works with multiple axes:\n\nd[None,None,:].shape\n\ntorch.Size([1, 1, 3])\n\n\nYou can always skip trailing :‘s, and’…’ means ‘all preceding dimensions’:\n\nd[None].shape, d[...,None].shape\n\n(torch.Size([1, 3]), torch.Size([3, 1]))\n\n\n\n\nCode\n\nd[:,None].expand_as(m)\n\ntensor([[10., 10., 10.],\n        [20., 20., 20.],\n        [30., 30., 30.]])\n\n\n\n\nExplanation\n\n{\n    \"d[:,None]\": \"Adds a dimension on the last axis, turning `d` into [10],[20],[30]\",\n    \".expand_as(m)\": \"When we expand now, the result will be a lateral expansion rather than vertical\"\n};\n\n\nd[:,None].expand_as(m)\n\ntensor([[10., 10., 10.],\n        [20., 20., 20.],\n        [30., 30., 30.]])\n\n\n\nm + d[:,None]\n\ntensor([[11., 12., 13.],\n        [24., 25., 26.],\n        [37., 38., 39.]])\n\n\nFrom here, we visualize this in excel. Follow the timestamp here\n\n\n\nmatmul with broadcasting\nWith this information now, we can use this to get rid of the loop:\n\nCode\n\ndef matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar,bc)\n    for i in range(ar):\n        # c[i,j] = (a[i,:] * b[:,j]).sum() previous\n        c[i] = (a[i].unsqueeze(-1) * b).sum(dim=0)\n        # This is equivalent to c[i,:]\n        # Rewritten in None form:\n        #c[i] = (a[i][:,None] * b).sum(dim=0)\n        # Rewritten again to avoid second index altogether:\n        #c[i] = (a[i,:,None] * b).sum(dim=0)\n    return c\n\n\n\nExplanation\n\n{\n    \"a[i].unsqueeze(-1)\": \"This takes `a` at `i` and expands its last dimension by 1, and now it's a rank 2 tensor\",\n    \"* b\": \"This newly reshaped array can then be multiplied by b properly without issue\",\n    \".sum(dim=0)\": \"And finally we can take the sum of that result, doing so on the first dimension\"\n};\n\n\nm2*m1[0].unsqueeze(-1)\n\ntensor([[-0., 0., 0.,  ..., 0., -0., 0.],\n        [-0., 0., 0.,  ..., -0., -0., -0.],\n        [0., 0., -0.,  ..., -0., 0., 0.],\n        ...,\n        [-0., 0., -0.,  ..., -0., 0., -0.],\n        [-0., 0., -0.,  ..., 0., -0., -0.],\n        [0., 0., 0.,  ..., 0., -0., -0.]])\n\n\n\nm1[0,:,None] * m2\n\ntensor([[-0., 0., 0.,  ..., 0., -0., 0.],\n        [-0., 0., 0.,  ..., -0., -0., -0.],\n        [0., 0., -0.,  ..., -0., 0., 0.],\n        ...,\n        [-0., 0., -0.,  ..., -0., 0., -0.],\n        [-0., 0., -0.,  ..., 0., -0., -0.],\n        [0., 0., 0.,  ..., 0., -0., -0.]])\n\n\n\nm1[0].unsqueeze(-1).shape\n\ntorch.Size([784, 1])\n\n\n\nassert m1[0].unsqueeze(-1).shape == m1[0][:,None].shape == m1[0,:,None].shape == m1[0,None].T.shape\n\n\n\n\n202 µs ± 66.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n445/.157\n\n2834.394904458599\n\n\nWe are now more than 2000x faster!\n\ntest_close(t1, matmul(m1,m2), eps=1e-4)\n\n\n\n\nBroadcasting Rules\n\nd\n\ntensor([10., 20., 30.])\n\n\n\n# Pad a dimension\nd[None,:], d[None,:].shape\n\n(tensor([[10., 20., 30.]]), torch.Size([1, 3]))\n\n\n\n# Pad the last dimension\nd[:,None].shape\n\ntorch.Size([3, 1])\n\n\n\n# Perform matmul\nd[None,:] * d[:,None]\n\ntensor([[100., 200., 300.],\n        [200., 400., 600.],\n        [300., 600., 900.]])\n\n\n\n# Peform broadcast\nd[None] &gt; d[:,None]\n\ntensor([[False,  True,  True],\n        [False, False,  True],\n        [False, False, False]])\n\n\n\n\nEinstein Summation\nRecall the inner most part of the for loops earlier:\nc[i,j] += a[i,k] * b[k,j]\nAnd when we removed this, it looked like so:\nc[i,j] = (a[i,:] * b[:,j]).sum()\nWe can rewrite this in Einstein Summation using the following steps:\n\nGet rid of the names of everything and the operators\n\ni,j += i,k k,j\n\nMove i,j to the end and make an arrow point at it:\n\ni,k k,j -&gt; i,j\n\nGet rid of the commas:\n\nik kj -&gt; ij\n\nReplace spaces with commas:\n\nik,kj-&gt;ij\n\nCode\n\ndef matmul(a,b): return torch.einsum('ik,kj-&gt;ij', a, b)\n\n\n\nExplaination\n\n{\n    \"-&gt;\": \"To the left of the arrow is the input, to the right of the arrow is the output\",\n    \"ik,kj\": \"Inputs are delimited by comma, so there are two in this case\",\n    \"ik\":\"Rank is denoted by the number of letters there are. `ik` and `kj` are both rank 2\",\n    \"kj\":\"These inputs are read (shape wise) as `k` by `j` or `i` by `k`\",\n    \"k\":\"When a letter is repeated across inputs, it is assumed to be a dot product along that dimension\",\n};\n\n\n\n\n101 µs ± 42.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\npytorch op\nSince we have now explored matmul to it’s fullest extent, we can utilize pytorch’s operator directly for matrix multiplication:\n\n\n\nThe slowest run took 19.75 times longer than the fastest. This could mean that an intermediate result is being cached.\n13.1 µs ± 22.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nThe matmul is pushed to a BLAS (basic linear algebra subprogram) cuBLAS for nvidia, ex. This is what the M1 has for example and how they entered the space.\nmatmul is so common and useful that it has it’s own operator, @:\n\nt2 = m1@m2\n\n\ntest_close(t1,t2)\n\nThis is the exact same speed as m1.matmul(m2)\n\n# from nbdev.export import notebook2script\n# notebook2script(\"matmul.ipynb\")\n\nConverted matmul.ipynb.",
    "crumbs": [
      "Lesson 1, Matrix Multiplication (part 1)"
    ]
  },
  {
    "objectID": "why_sqrt5.html",
    "href": "why_sqrt5.html",
    "title": "Why do we use sqrt5?",
    "section": "",
    "text": "In torch.nn.modules.conv’s reset_parameters function (the initialization function), init.kaiming_uniform_ is used (what we learned last lecture) with the following setting:\ndef reset_parameters(self):\n    ...\n    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n    ...\nThis a is undocumented.\n\nsource\n\n\n\n normalize (x, mean, std)\n\n\nsource\n\n\n\n\n get_data ()\n\n\nx_train, y_train, x_valid, y_valid = get_data()\ntrain_mean, train_std = x_train.mean(), x_train.std()\n# Use functools.partial here to make it a bit more efficient\nnorm = partial(normalize, mean=train_mean, std=train_std)\nx_train = norm(x_train)\nx_valid = norm(x_valid)\n\n\nx_train = x_train.view(-1,1,28,28)\nx_valid = x_valid.view(-1,1,28,28)\nx_train.shape, x_valid.shape\n\n(torch.Size([50000, 1, 28, 28]), torch.Size([10000, 1, 28, 28]))\n\n\nTo do a convolution we need a square or rectangular shape, which is why the data is now batch_size, n_channels, width, height\n\nCodeCode + Explanation\n\n\n\nnum_datapoints,*_ = x_train.shape\nnum_classes = y_train.max()+1\nnum_hidden = 32\nnum_datapoints, num_classes\n\n(50000, tensor(10))\n\n\n\n\n\nnum_datapoints,*_ = x_train.shape\nnum_classes = y_train.max()+1\nnum_hidden = 32\nnum_datapoints, num_classes\n\n(50000, tensor(10))\n\n\n\nnum_datapoints,*_ = x_train.shape\n\nThis is some python code that will only keep the first out of a tuple and forgo the rest\n\n\nnum_classes = y_train.max()+1\n\nGet the number of classes\n\n\nnum_hidden = 32\n\nThe size of our hidden layer\n\n\n\n\n\nfrom torch import nn\n\nNow we’ll create a simple nn.Conv2d layer that expects a single-channel input, the size of the hidden layer, and we’ll make it a 5x5 kernel (more on this later):\n\nl1 = nn.Conv2d(1, num_hidden, 5)\n\n\nx = x_valid[:100] # Get a subset of our data\n\n\nx.shape\n\ntorch.Size([100, 1, 28, 28])\n\n\n\ndef stats(x:tensor):\n    \"Return the mean and std of x\"\n    return x.mean(), x.std()\n\n\nl1.weight.shape\n\ntorch.Size([32, 1, 5, 5])\n\n\n\n32: output shape\n1: one input filter/channel\n5x5: the kernel shape\n\n\nstats(l1.weight), stats(l1.bias)\n\n((tensor(0.0024, grad_fn=&lt;MeanBackward0&gt;),\n  tensor(0.1156, grad_fn=&lt;StdBackward0&gt;)),\n (tensor(0.0272, grad_fn=&lt;MeanBackward0&gt;),\n  tensor(0.1088, grad_fn=&lt;StdBackward0&gt;)))\n\n\n\nt = l1(x)\n\n\nstats(t) # want this to be as close to a mean of 0 and std of 1.\n\n(tensor(0.0331, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.5936, grad_fn=&lt;StdBackward0&gt;))\n\n\nWhat happens if we use init._kaiming_normal_?\nKaiming is designed to be used after a leaky ReLU:\n\n\n\nimage.png\n\n\nLeaky just means that some value a is equal to the slope of the negative numbers from -inf -&gt; 0\nBut since we are working with a conv layer, instead it is just a straight line so our leak (a) is 1 effectively\n\nfrom torch.nn import init\n\n\ninit.kaiming_normal_(l1.weight, a=1.) # Because no ReLU\nstats(l1(x))\n\n(tensor(0.0619, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.1906, grad_fn=&lt;StdBackward0&gt;))\n\n\nKaiming got us close to 0,1. and seems to be working quite well. What happens when we use leaky relu\n\nimport torch.nn.functional as F\n\n\ndef f1(x, a=0.): return F.leaky_relu(l1(x), a)\n\n\ninit.kaiming_normal_(l1.weight, a=0)\nstats(f1(x))\n\n(tensor(0.5356, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.9657, grad_fn=&lt;StdBackward0&gt;))\n\n\nWhile leaky relu with the default keeps the std at 1, our mean is now half due to getting rid of half the values (those in negative).\nWhat happens if we use the torch default?\n\nl1 = nn.Conv2d(1, num_hidden, 5)\nstats(f1(x))\n\n(tensor(0.2058, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.3633, grad_fn=&lt;StdBackward0&gt;))\n\n\nOur stats are infinitly worse now, no where close to the hoped mean or std.\nWhat happens if we have a varience &lt; 1?\nTo get the number of matrix multiplications that occur in a conv layer, we need to multiply the output shape by the filter matrix:\n\nl1.weight.shape\n\ntorch.Size([32, 1, 5, 5])\n\n\n\n32*5*5 # 800 total matrix multiplications for this individual layer\n\n800\n\n\n\nCodeCode + Explanation\n\n\n\nreceptive_field_size = l1.weight[0,0].numel()\nreceptive_field_size, l1.weight[0,0].shape\n\n(25, torch.Size([5, 5]))\n\n\n\n\n\nreceptive_field_size = l1.weight[0,0].numel()\nreceptive_field_size, l1.weight[0,0].shape\n\n(25, torch.Size([5, 5]))\n\n\n\nl1.weight[0,0]\n\nGrab the first matrix in the conv\n\n\n.numel()\n\nGet the total size of that matrix’s individual squares\n\n\nreceptive_field_size\n\nHow many elements are in the kernel\n\n\n\n\n\nnum_output_filters, num_input_filters, *_ = l1.weight.shape\nnum_output_filters, num_input_filters\n\n(32, 1)\n\n\n\nfan_in = num_input_filters * receptive_field_size\nfan_out = num_output_filters * receptive_field_size\nfan_in, fan_out\n\n(25, 800)\n\n\n\ndef gain(a:float):\n    \"Calculates the size of the gain during kaiming init\"\n    return math.sqrt(2. / (1+a**2))\n\n\ngain(1), gain(0), gain(0.01), gain(0.1), gain(math.sqrt(5))\n\n(1.0,\n 1.4142135623730951,\n 1.4141428569978354,\n 1.4071950894605838,\n 0.5773502691896257)\n\n\nWith a slope of 1, the gain is 1 as it’s linear. With a slope of &lt; 1, it will approach root 2:\n\nmath.sqrt(2)\n\n1.4142135623730951\n\n\nWith sqrt(5) it is far away from the gain we were expecting, which isn’t good.\nHowever it doesn’t use kaiming normal, it uses kaiming uniform.\n\nKey: Blue is normal, Red is uniform\n\n\n\n\nimage.png\n\n\nWhat is the std of a uniform distribution?\n\nimport torch\ntorch.zeros(10_000).uniform_(-1,1).std()\n\ntensor(0.5787)\n\n\nIt’s std is .57, or 1/sqrt(3.):\n\n1/math.sqrt(3.)\n\n0.5773502691896258\n\n\n\nCodeCode + Explanation\n\n\n\n# Refactor into our own\ndef kaiming_v2(x, a, use_fan_out=False):\n    num_out_filters, num_input_filters, *_ = x.shape\n    receptive_field_size = x[0,0].shape.numel()\n    if use_fan_out:\n        fan = num_out_filters * receptive_field_size\n    else:\n        fan = num_input_filters * receptive_field_size\n    std = gain(a) / math.sqrt(fan)\n    bound = math.sqrt(3.) * std\n    x.data.uniform_(-bound, bound)\n\n\n\n\n# Refactor into our own\ndef kaiming_v2(x, a, use_fan_out=False):\n    num_out_filters, num_input_filters, *_ = x.shape\n    receptive_field_size = x[0,0].shape.numel()\n    if use_fan_out:\n        fan = num_out_filters * receptive_field_size\n    else:\n        fan = num_input_filters * receptive_field_size\n    std = gain(a) / math.sqrt(fan)\n    bound = math.sqrt(3.) * std\n    x.data.uniform_(-bound, bound)\n\n\nreceptive_field_size = x[0,0].shape.numel()\n\nCalculate the total squares in our usable matrix\n\n\n    std = gain(a) / math.sqrt(fan)\n\nCalculate the standard deviation of a based on fan\n\n\n    x.data.uniform_(-bound, bound)\n\nApply the newfound bounds to the data inplace\n\n\n\n\n\nkaiming_v2(l1.weight, a=0)\nstats(f1(x))\n\n(tensor(0.5162, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.8743, grad_fn=&lt;StdBackward0&gt;))\n\n\nVarience of about 1, and a mean of .5, the expected, What happens if I do it with sqrt(5)?\n\nkaiming_v2(l1.weight, a=5)\nstats(f1(x))\n\n(tensor(0.0986, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.1791, grad_fn=&lt;StdBackward0&gt;))\n\n\nWe’d expect to get the same as the pytorch default, which we have done. But what does this really look like?\n\nclass Flatten(nn.Module):\n    \"A small layer which will flatten `x` by the last axis\"\n    def forward(self, x): \n        return x.view(-1)\n\n\ndef get_model():\n    m = nn.Sequential(\n        nn.Conv2d(1,8,5, stride=2, padding=2), nn.ReLU(),\n        nn.Conv2d(8,16,3, stride=2, padding=1), nn.ReLU(),\n        nn.Conv2d(16,32,3, stride=2, padding=1), nn.ReLU(),\n        nn.Conv2d(32, 1, 3, stride=2, padding=1),\n        nn.AdaptiveAvgPool2d(1),\n        Flatten()\n    )\n    return m\n\nm = get_model()\n\nWe create a super small test model of 4 conv layers + ReLU with a pooling layer and flattening.\n\ny = y_valid[:100].float() # Grab the labels for our subset of `x`\n\nNext we run it through the whole convnet and take the stats of our result:\n\nt = m(x)\nstats(t)\n\n(tensor(0.0161, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.0103, grad_fn=&lt;StdBackward0&gt;))\n\n\nWhen using the default PyTorch init, the varience is almost 0. The first layer and last layers now have a huge difference.\n\nloss = mse(t,y)\nloss.backward()\n\n\nstats(m[0].weight.grad)\n\n(tensor(-0.0114), tensor(0.0453))\n\n\nPost backward, std of the weights is still nowhere near one.\nWhat happens if we use kaiming uniform?\n\nm = get_model()\n\n\nCodeCode + Explanation\n\n\n\nfor layer in m:\n    if isinstance(layer, nn.Conv2d):\n        init.kaiming_uniform_(layer.weight)\n        layer.bias.data.zero_()\n\n\n\n\nfor layer in m:\n    if isinstance(layer, nn.Conv2d):\n        init.kaiming_uniform_(layer.weight)\n        layer.bias.data.zero_()\n\n\nmultline\n\nIf it’s a conv layer, initialize with kaiming uniform\n\n\n        layer.bias.data.zero_()\n\nAfterwards the bias of the data is zeroed out\n\n\n\n\n\nt = m(x)\nstats(t)\n\n(tensor(-0.0386, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.3797, grad_fn=&lt;StdBackward0&gt;))\n\n\nIt’s not terrible, much better than the .001 we had earlier. What happens after the backward?\n\nloss = mse(t,y)\nloss.backward()\nstats(m[0].weight.grad)\n\n(tensor(-0.1251), tensor(0.6223))\n\n\nAre stats are now doing much better than before, with a mean of 0 and a std approaching .5 much more.\nFrom here, read This Notebook and come back",
    "crumbs": [
      "Why do we use sqrt5?"
    ]
  },
  {
    "objectID": "why_sqrt5.html#does-nn.conv2d-init-work-well",
    "href": "why_sqrt5.html#does-nn.conv2d-init-work-well",
    "title": "Why do we use sqrt5?",
    "section": "",
    "text": "In torch.nn.modules.conv’s reset_parameters function (the initialization function), init.kaiming_uniform_ is used (what we learned last lecture) with the following setting:\ndef reset_parameters(self):\n    ...\n    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n    ...\nThis a is undocumented.\n\nsource\n\n\n\n normalize (x, mean, std)\n\n\nsource\n\n\n\n\n get_data ()\n\n\nx_train, y_train, x_valid, y_valid = get_data()\ntrain_mean, train_std = x_train.mean(), x_train.std()\n# Use functools.partial here to make it a bit more efficient\nnorm = partial(normalize, mean=train_mean, std=train_std)\nx_train = norm(x_train)\nx_valid = norm(x_valid)\n\n\nx_train = x_train.view(-1,1,28,28)\nx_valid = x_valid.view(-1,1,28,28)\nx_train.shape, x_valid.shape\n\n(torch.Size([50000, 1, 28, 28]), torch.Size([10000, 1, 28, 28]))\n\n\nTo do a convolution we need a square or rectangular shape, which is why the data is now batch_size, n_channels, width, height\n\nCodeCode + Explanation\n\n\n\nnum_datapoints,*_ = x_train.shape\nnum_classes = y_train.max()+1\nnum_hidden = 32\nnum_datapoints, num_classes\n\n(50000, tensor(10))\n\n\n\n\n\nnum_datapoints,*_ = x_train.shape\nnum_classes = y_train.max()+1\nnum_hidden = 32\nnum_datapoints, num_classes\n\n(50000, tensor(10))\n\n\n\nnum_datapoints,*_ = x_train.shape\n\nThis is some python code that will only keep the first out of a tuple and forgo the rest\n\n\nnum_classes = y_train.max()+1\n\nGet the number of classes\n\n\nnum_hidden = 32\n\nThe size of our hidden layer\n\n\n\n\n\nfrom torch import nn\n\nNow we’ll create a simple nn.Conv2d layer that expects a single-channel input, the size of the hidden layer, and we’ll make it a 5x5 kernel (more on this later):\n\nl1 = nn.Conv2d(1, num_hidden, 5)\n\n\nx = x_valid[:100] # Get a subset of our data\n\n\nx.shape\n\ntorch.Size([100, 1, 28, 28])\n\n\n\ndef stats(x:tensor):\n    \"Return the mean and std of x\"\n    return x.mean(), x.std()\n\n\nl1.weight.shape\n\ntorch.Size([32, 1, 5, 5])\n\n\n\n32: output shape\n1: one input filter/channel\n5x5: the kernel shape\n\n\nstats(l1.weight), stats(l1.bias)\n\n((tensor(0.0024, grad_fn=&lt;MeanBackward0&gt;),\n  tensor(0.1156, grad_fn=&lt;StdBackward0&gt;)),\n (tensor(0.0272, grad_fn=&lt;MeanBackward0&gt;),\n  tensor(0.1088, grad_fn=&lt;StdBackward0&gt;)))\n\n\n\nt = l1(x)\n\n\nstats(t) # want this to be as close to a mean of 0 and std of 1.\n\n(tensor(0.0331, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.5936, grad_fn=&lt;StdBackward0&gt;))\n\n\nWhat happens if we use init._kaiming_normal_?\nKaiming is designed to be used after a leaky ReLU:\n\n\n\nimage.png\n\n\nLeaky just means that some value a is equal to the slope of the negative numbers from -inf -&gt; 0\nBut since we are working with a conv layer, instead it is just a straight line so our leak (a) is 1 effectively\n\nfrom torch.nn import init\n\n\ninit.kaiming_normal_(l1.weight, a=1.) # Because no ReLU\nstats(l1(x))\n\n(tensor(0.0619, grad_fn=&lt;MeanBackward0&gt;),\n tensor(1.1906, grad_fn=&lt;StdBackward0&gt;))\n\n\nKaiming got us close to 0,1. and seems to be working quite well. What happens when we use leaky relu\n\nimport torch.nn.functional as F\n\n\ndef f1(x, a=0.): return F.leaky_relu(l1(x), a)\n\n\ninit.kaiming_normal_(l1.weight, a=0)\nstats(f1(x))\n\n(tensor(0.5356, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.9657, grad_fn=&lt;StdBackward0&gt;))\n\n\nWhile leaky relu with the default keeps the std at 1, our mean is now half due to getting rid of half the values (those in negative).\nWhat happens if we use the torch default?\n\nl1 = nn.Conv2d(1, num_hidden, 5)\nstats(f1(x))\n\n(tensor(0.2058, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.3633, grad_fn=&lt;StdBackward0&gt;))\n\n\nOur stats are infinitly worse now, no where close to the hoped mean or std.\nWhat happens if we have a varience &lt; 1?\nTo get the number of matrix multiplications that occur in a conv layer, we need to multiply the output shape by the filter matrix:\n\nl1.weight.shape\n\ntorch.Size([32, 1, 5, 5])\n\n\n\n32*5*5 # 800 total matrix multiplications for this individual layer\n\n800\n\n\n\nCodeCode + Explanation\n\n\n\nreceptive_field_size = l1.weight[0,0].numel()\nreceptive_field_size, l1.weight[0,0].shape\n\n(25, torch.Size([5, 5]))\n\n\n\n\n\nreceptive_field_size = l1.weight[0,0].numel()\nreceptive_field_size, l1.weight[0,0].shape\n\n(25, torch.Size([5, 5]))\n\n\n\nl1.weight[0,0]\n\nGrab the first matrix in the conv\n\n\n.numel()\n\nGet the total size of that matrix’s individual squares\n\n\nreceptive_field_size\n\nHow many elements are in the kernel\n\n\n\n\n\nnum_output_filters, num_input_filters, *_ = l1.weight.shape\nnum_output_filters, num_input_filters\n\n(32, 1)\n\n\n\nfan_in = num_input_filters * receptive_field_size\nfan_out = num_output_filters * receptive_field_size\nfan_in, fan_out\n\n(25, 800)\n\n\n\ndef gain(a:float):\n    \"Calculates the size of the gain during kaiming init\"\n    return math.sqrt(2. / (1+a**2))\n\n\ngain(1), gain(0), gain(0.01), gain(0.1), gain(math.sqrt(5))\n\n(1.0,\n 1.4142135623730951,\n 1.4141428569978354,\n 1.4071950894605838,\n 0.5773502691896257)\n\n\nWith a slope of 1, the gain is 1 as it’s linear. With a slope of &lt; 1, it will approach root 2:\n\nmath.sqrt(2)\n\n1.4142135623730951\n\n\nWith sqrt(5) it is far away from the gain we were expecting, which isn’t good.\nHowever it doesn’t use kaiming normal, it uses kaiming uniform.\n\nKey: Blue is normal, Red is uniform\n\n\n\n\nimage.png\n\n\nWhat is the std of a uniform distribution?\n\nimport torch\ntorch.zeros(10_000).uniform_(-1,1).std()\n\ntensor(0.5787)\n\n\nIt’s std is .57, or 1/sqrt(3.):\n\n1/math.sqrt(3.)\n\n0.5773502691896258\n\n\n\nCodeCode + Explanation\n\n\n\n# Refactor into our own\ndef kaiming_v2(x, a, use_fan_out=False):\n    num_out_filters, num_input_filters, *_ = x.shape\n    receptive_field_size = x[0,0].shape.numel()\n    if use_fan_out:\n        fan = num_out_filters * receptive_field_size\n    else:\n        fan = num_input_filters * receptive_field_size\n    std = gain(a) / math.sqrt(fan)\n    bound = math.sqrt(3.) * std\n    x.data.uniform_(-bound, bound)\n\n\n\n\n# Refactor into our own\ndef kaiming_v2(x, a, use_fan_out=False):\n    num_out_filters, num_input_filters, *_ = x.shape\n    receptive_field_size = x[0,0].shape.numel()\n    if use_fan_out:\n        fan = num_out_filters * receptive_field_size\n    else:\n        fan = num_input_filters * receptive_field_size\n    std = gain(a) / math.sqrt(fan)\n    bound = math.sqrt(3.) * std\n    x.data.uniform_(-bound, bound)\n\n\nreceptive_field_size = x[0,0].shape.numel()\n\nCalculate the total squares in our usable matrix\n\n\n    std = gain(a) / math.sqrt(fan)\n\nCalculate the standard deviation of a based on fan\n\n\n    x.data.uniform_(-bound, bound)\n\nApply the newfound bounds to the data inplace\n\n\n\n\n\nkaiming_v2(l1.weight, a=0)\nstats(f1(x))\n\n(tensor(0.5162, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.8743, grad_fn=&lt;StdBackward0&gt;))\n\n\nVarience of about 1, and a mean of .5, the expected, What happens if I do it with sqrt(5)?\n\nkaiming_v2(l1.weight, a=5)\nstats(f1(x))\n\n(tensor(0.0986, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.1791, grad_fn=&lt;StdBackward0&gt;))\n\n\nWe’d expect to get the same as the pytorch default, which we have done. But what does this really look like?\n\nclass Flatten(nn.Module):\n    \"A small layer which will flatten `x` by the last axis\"\n    def forward(self, x): \n        return x.view(-1)\n\n\ndef get_model():\n    m = nn.Sequential(\n        nn.Conv2d(1,8,5, stride=2, padding=2), nn.ReLU(),\n        nn.Conv2d(8,16,3, stride=2, padding=1), nn.ReLU(),\n        nn.Conv2d(16,32,3, stride=2, padding=1), nn.ReLU(),\n        nn.Conv2d(32, 1, 3, stride=2, padding=1),\n        nn.AdaptiveAvgPool2d(1),\n        Flatten()\n    )\n    return m\n\nm = get_model()\n\nWe create a super small test model of 4 conv layers + ReLU with a pooling layer and flattening.\n\ny = y_valid[:100].float() # Grab the labels for our subset of `x`\n\nNext we run it through the whole convnet and take the stats of our result:\n\nt = m(x)\nstats(t)\n\n(tensor(0.0161, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.0103, grad_fn=&lt;StdBackward0&gt;))\n\n\nWhen using the default PyTorch init, the varience is almost 0. The first layer and last layers now have a huge difference.\n\nloss = mse(t,y)\nloss.backward()\n\n\nstats(m[0].weight.grad)\n\n(tensor(-0.0114), tensor(0.0453))\n\n\nPost backward, std of the weights is still nowhere near one.\nWhat happens if we use kaiming uniform?\n\nm = get_model()\n\n\nCodeCode + Explanation\n\n\n\nfor layer in m:\n    if isinstance(layer, nn.Conv2d):\n        init.kaiming_uniform_(layer.weight)\n        layer.bias.data.zero_()\n\n\n\n\nfor layer in m:\n    if isinstance(layer, nn.Conv2d):\n        init.kaiming_uniform_(layer.weight)\n        layer.bias.data.zero_()\n\n\nmultline\n\nIf it’s a conv layer, initialize with kaiming uniform\n\n\n        layer.bias.data.zero_()\n\nAfterwards the bias of the data is zeroed out\n\n\n\n\n\nt = m(x)\nstats(t)\n\n(tensor(-0.0386, grad_fn=&lt;MeanBackward0&gt;),\n tensor(0.3797, grad_fn=&lt;StdBackward0&gt;))\n\n\nIt’s not terrible, much better than the .001 we had earlier. What happens after the backward?\n\nloss = mse(t,y)\nloss.backward()\nstats(m[0].weight.grad)\n\n(tensor(-0.1251), tensor(0.6223))\n\n\nAre stats are now doing much better than before, with a mean of 0 and a std approaching .5 much more.\nFrom here, read This Notebook and come back",
    "crumbs": [
      "Why do we use sqrt5?"
    ]
  },
  {
    "objectID": "why_sqrt5.html#to-twitter-we-go",
    "href": "why_sqrt5.html#to-twitter-we-go",
    "title": "Why do we use sqrt5?",
    "section": "To twitter we go",
    "text": "To twitter we go\nJeremy pinged on twitter asking why this exists. Soumith Chintala answered that this was a historical accident that was never published, but was always in the torch code for the last ~15 years as it was deemed a good bug.\nAfter Jeremy pointed out the issue, the torch team opened an issue to fix it.\nMoral of the story:\n\nDon’t blindly trust a popular library’s thing, ask questions and run analysis. It could be a bug that’s negatively impacting performance without realizing it",
    "crumbs": [
      "Why do we use sqrt5?"
    ]
  },
  {
    "objectID": "fully_connected.html",
    "href": "fully_connected.html",
    "title": "Lesson 1, The forward and backward passes (part 2)",
    "section": "",
    "text": "source\n\n\n\n normalize (x, mean, std)\n\n\nsource\n\n\n\n\n get_data ()\n\n\nx_train,y_train,x_valid,y_valid = get_data()\ntrain_mean,train_std = x_train.mean(), x_train.std()\ntrain_mean,train_std\n\n(tensor(0.1304), tensor(0.3073))\n\n\nWe need to normalize our data (mean ~= 0, std ~=1) by the training data, so they are on the same scale. If we did not then they could be considered two completely different datasets as a whole, and not actually part of the same bunch\n\nx_train = normalize(x_train, train_mean, train_std)\nx_valid = normalize(x_valid, train_mean, train_std)\n\n\ntrain_mean,train_std = x_train.mean(), x_train.std()\ntrain_mean,train_std\n\n(tensor(2.1425e-08), tensor(1.))\n\n\n\nsource\n\n\n\n\n test_near_zero (a, tol=0.001)\n\n\ntest_near_zero(x_train.mean())\ntest_near_zero(1-x_train.std())\n\n\n\n\nn,m = x_train.shape\nc = y_train.max()+1\n\n\n\n\n\n{\n    \"n\":\"Size of the training set\",\n    \"m\":\"The length of one input\",\n    \"c\":\"Number of activations eventual to classify with\"\n};\n\n\nn,m,c\n\n(50000, 784, tensor(10))",
    "crumbs": [
      "Lesson 1, The forward and backward passes (part 2)"
    ]
  },
  {
    "objectID": "fully_connected.html#initial-helpers",
    "href": "fully_connected.html#initial-helpers",
    "title": "Lesson 1, The forward and backward passes (part 2)",
    "section": "",
    "text": "source\n\n\n\n normalize (x, mean, std)\n\n\nsource\n\n\n\n\n get_data ()\n\n\nx_train,y_train,x_valid,y_valid = get_data()\ntrain_mean,train_std = x_train.mean(), x_train.std()\ntrain_mean,train_std\n\n(tensor(0.1304), tensor(0.3073))\n\n\nWe need to normalize our data (mean ~= 0, std ~=1) by the training data, so they are on the same scale. If we did not then they could be considered two completely different datasets as a whole, and not actually part of the same bunch\n\nx_train = normalize(x_train, train_mean, train_std)\nx_valid = normalize(x_valid, train_mean, train_std)\n\n\ntrain_mean,train_std = x_train.mean(), x_train.std()\ntrain_mean,train_std\n\n(tensor(2.1425e-08), tensor(1.))\n\n\n\nsource\n\n\n\n\n test_near_zero (a, tol=0.001)\n\n\ntest_near_zero(x_train.mean())\ntest_near_zero(1-x_train.std())\n\n\n\n\nn,m = x_train.shape\nc = y_train.max()+1\n\n\n\n\n\n{\n    \"n\":\"Size of the training set\",\n    \"m\":\"The length of one input\",\n    \"c\":\"Number of activations eventual to classify with\"\n};\n\n\nn,m,c\n\n(50000, 784, tensor(10))",
    "crumbs": [
      "Lesson 1, The forward and backward passes (part 2)"
    ]
  },
  {
    "objectID": "fully_connected.html#foundations-version",
    "href": "fully_connected.html#foundations-version",
    "title": "Lesson 1, The forward and backward passes (part 2)",
    "section": "Foundations version",
    "text": "Foundations version\n\nBasic architecture\n\nOne hidden layer\nMean squared error to keep things simplified rather than cross entropy\n\nWe initialize with a simplified version of kaiming init / he init\n\nCode\n\nnh = 50\nw1 = torch.randn(m,nh)/math.sqrt(m)\nb1 = torch.zeros(nh)\nw2 = torch.randn(nh,1)/math.sqrt(nh)\nb2 = torch.zeros(1)\n\n\n\nExplanation\n\n{\n    \"nh\":\"The size of our fully-connected hidden layer (nodes)\",\n    \"w1\":\"One weight for our model, the first layer initialized (784,50)\",\n    \"b1\":\"The bias for that weight\",\n    \"w2\":\"Another weight for our model, the second layer (50,1)\",\n    \"b2\":\"The bias for that weight\",\n    \"torch.randn(a,b)/math.sqrt(a)\":\"Simplified kaiming init/he init\"\n};\n\n\nw1.shape, b1.shape, w2.shape, b2.shape\n\n(torch.Size([784, 50]), torch.Size([50]), torch.Size([50, 1]), torch.Size([1]))\n\n\n\ntest_near_zero(w1.mean())\ntest_near_zero(w1.std()-1/math.sqrt(m))\n\n\n# This should be ~ (0,1) (mean,std)\nx_valid.mean(),x_valid.std()\n\n(tensor(-0.0059), tensor(0.9924))\n\n\n\ndef lin(inp, weight, bias): return inp@weight + bias\n\n\nt = lin(x_valid, w1, b1)\n\n\n# So should this because we used kaiming init which is designed to have this effect\nt.mean(), t.std()\n\n(tensor(-0.0417), tensor(1.0341))\n\n\n\n\nCode\n\ndef relu(inp): return inp.clamp_min(0.)\n\n\n\nExplanation\n\n{\n    \".clamp_min\":\"A ReLU activation will turn all negatives into zero\"\n};\n\n\nWhile there are other ways of writing that, if you can find a function attached to a tensor for the thing you want to do, it will almost always be faster because it will be written in C - Jeremy Howard\n\n\nt = relu(lin(x_valid, w1, b1))\n\n\nt.mean(), t.std()\n\n(tensor(0.3898), tensor(0.5947))\n\n\nUh oh! What went wrong?\n\nWhiteboard session stats at 1:31:00, YouTube link\n\nBasically we took everything with a mean below zero and just got rid of it. As a result we lost a ton of good data points, and our standard deviation and mean drastically swong as a result.\n\\[\\operatorname{std}=\\sqrt{\\frac{2}{\\left(1+a^2\\right) \\times \\text { fan_in }}}\\]\nSolution is to stick a two on the top:\n\nstd = math.sqrt(2/m)\n\n\nw1 = torch.randn(m,nh)*std\nt = relu(lin(x_valid, w1,b1))\n\nt.mean(), t.std()\n\n(tensor(0.5535), tensor(0.8032))\n\n\nWhile this solved the standard deviation, our mean is now half because we still deleted everything below the mean\n\n# What if...?\ndef relu_v2(x): return x.clamp_min(0.) - 0.5\ndef relu_v3(x): return (torch.pow(x.clamp_min(0.), 0.9)) - 0.5\n\n\nw1 = torch.randn(m,nh)*std\nt = relu_v2(lin(x_valid, w1,b1))\n\nt.mean(), t.std()\n\n(tensor(0.0372), tensor(0.8032))\n\n\n\nt = relu_v3(lin(x_valid, w1,b1))\n\nt.mean(), t.std()\n\n(tensor(0.0181), tensor(0.7405))\n\n\nJeremy tried seeing just what would happen if during relu we reduced it by .5, and it seems to have helped some in returning us to the correct mean:\nHow well does this work in practice? – To test, I should try building a very basic CNN and throw it to ImageWoof and the only variance being the ReLU layer being utilized.\n\nw1 = torch.zeros(m,nh)\ninit.kaiming_normal_(w1, mode=\"fan_out\")\nt = relu(lin(x_valid, w1, b1))\n\n\nw1.mean(),w1.std()\n\n(tensor(9.4735e-05), tensor(0.0506))\n\n\n\nt.mean(),t.std()\n\n(tensor(0.4818), tensor(0.7318))\n\n\n\nw1 = torch.randn(m,nh)*math.sqrt(2./m)\nt = relu_v2(lin(x_valid, w1,b1))\n\nt.mean(), t.std()\n\n(tensor(-0.0279), tensor(0.7500))\n\n\n\nt = relu_v3(lin(x_valid, w1,b1))\n\nt.mean(), t.std()\n\n(tensor(-0.0422), tensor(0.6948))\n\n\n\ndef model(xb, v2=True):\n    l1 = lin(xb, w1, b1)\n    l2 = relu_v2(l1) if v2 else relu_v3(l1)\n    l3 = lin(l2, w2, b2)\n    return l3\n\n\n\n\n2.56 ms ± 578 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\n\n3.3 ms ± 104 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nassert model(x_valid).shape == torch.Size([x_valid.shape[0],1])",
    "crumbs": [
      "Lesson 1, The forward and backward passes (part 2)"
    ]
  },
  {
    "objectID": "fully_connected.html#loss-function-mse",
    "href": "fully_connected.html#loss-function-mse",
    "title": "Lesson 1, The forward and backward passes (part 2)",
    "section": "Loss function: MSE",
    "text": "Loss function: MSE\n\nmodel(x_valid).shape\n\ntorch.Size([10000, 1])\n\n\n\nCode\n\nsource\n\n\nmse\n\n mse (output, targ)\n\n\nExplanation\n\n{\n    \".squeeze()\":\"Opposite of unsqueeze, removes a dimension. We use it to remove the trailing `[1]`\"\n};\n\n\nNote: better to use -1 or 1 than just to do squeeze()\n\n\ny_train,y_valid = y_train.float(),y_valid.float()\n\n\npreds_a = model(x_train)\npreds_b = model(x_train,False)\n\n\npreds_a.shape\n\ntorch.Size([50000, 1])\n\n\n\nmse(preds_a, y_train)\n\ntensor(28.0614)\n\n\n\nmse(preds_b, y_train)\n\ntensor(27.8693)",
    "crumbs": [
      "Lesson 1, The forward and backward passes (part 2)"
    ]
  },
  {
    "objectID": "fully_connected.html#gradients-and-backward-pass",
    "href": "fully_connected.html#gradients-and-backward-pass",
    "title": "Lesson 1, The forward and backward passes (part 2)",
    "section": "Gradients and backward pass",
    "text": "Gradients and backward pass\nChain rule, chain rule, chain rule!\nStart with our last function and go backwards:\n\nCode\n\ndef mse_grad(inp, targ):\n    # grad of loss with respect to output of previous layer\n    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]\n\n\n\nExplanation\n\n{\n    \"inp.g\": \"Gradients need to be attached to the inputs, so it can be passed across all of the functions and utilized as the output of the previous layer is the input for the current layer\",\n    \"2 * (inp - targ)/len(inp)\": \"This is the derivitive of (inp-targ)^2/len(inp)\"\n};\n\n\n\nCode\n\ndef relu_grad(inp, out):\n    # grad of relu with respect to input activations\n    inp.g = (inp&gt;0).float() * out.g\n\n\n\nExplanation\n\n{\n    \"(inp&gt;0).float() * out.g\": \"The inp&gt;0 is familiar, but given *respect* we need to multiply it by the previous layer's gradients\",\n    \"inp&gt;0\":\"Given that anything negative after a ReLU is set to 0, it has no slope and thus a derivitive of 0. We take everything above 0 as a result\"\n};\n\n\n\nCode\n\ndef lin_grad(inp, out, w, b):\n    # grad of matmul with respect to input\n    inp.g = out.g @ w.t() # transpose\n    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n    b.g = out.g.sum(0)\n\n\n\nExplanation\n\n{\n    \"out.g @ w.t()\": \"The gradient of a matrix product is the product of the matrix transpose\",\n    \"w.g\": \"We need the outputs with respect to the weights\",\n    \"b.g.\": \"And we also need the outputs with respect to the biases\"\n};\n\n\n\nCode\n\ndef forward_and_backward(inp, targ):\n    # forward pass:\n    l1 = inp @ w1 + b1\n    l2 = relu_v2(l1)\n    out = l2 @ w2 + b2\n    # We don't actually need the loss in backward\n    loss = mse(out, targ)\n    \n    # backward pass:\n    mse_grad(out, targ)\n    lin_grad(l2, out, w2, b2)\n    relu_grad(l1, l2)\n    lin_grad(inp, l1, w1, b1)\n\n\n\nExplanation\n\n{\n    \"l1 = inp @ w1 + b1\\nlin_grad(inp, l1, w1, b1)\": \"The inputs to the gradients is the original input, the output, and the rest of the options passed originally\",\n    \"l2 = relu_v2(l1)\\nrelu_grad(l1, l2)\":\"This pattern continues until we start and end on the original linear layer, traveling through the model and loss function twice\"\n};\n\nBackprop is the chain rule, with making sure all the calculations are saved somewhere\n\nforward_and_backward(x_train, y_train)\n\n\n# Save for testing against later\nw1g = w1.g.clone()\nw2g = w2.g.clone()\nb1g = b1.g.clone()\nb2g = b2.g.clone()\nig = x_train.g.clone()\n\nAnd now we cheat with pytorch autograd to check results:\n\nxt2 = x_train.clone().requires_grad_(True)\nw12 = w1.clone().requires_grad_(True)\nw22 = w2.clone().requires_grad_(True)\nb12 = b1.clone().requires_grad_(True)\nb22 = b2.clone().requires_grad_(True)\n\n\ndef forward(inp, targ):\n    # forward pass\n    l1 = inp @ w12 + b12\n    l2 = relu_v2(l1)\n    out = l2 @ w22 + b22\n    return mse(out, targ)\n\n\nloss = forward(xt2, y_train)\n\n\nloss.backward()\n\n\n# And now test\ntest_close(w22.grad, w2g)\ntest_close(b22.grad, b2g)\ntest_close(w12.grad, w1g)\ntest_close(b12.grad, b1g)\ntest_close(xt2.grad, ig)",
    "crumbs": [
      "Lesson 1, The forward and backward passes (part 2)"
    ]
  },
  {
    "objectID": "fully_connected.html#layers-as-classes",
    "href": "fully_connected.html#layers-as-classes",
    "title": "Lesson 1, The forward and backward passes (part 2)",
    "section": "Layers as classes",
    "text": "Layers as classes\n\nCode\n\nclass ReLU():\n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp.clamp_min(0.)-0.5\n        return self.out\n    \n    def backward(self): \n        self.inp.g = (self.inp&gt;0).float() * self.out.g\n\n\n\nExplanation\n\n{\n    \"def __call__(self, inp)\": \"Let's the class be called with ReLU()() and perform an operation\",\n    \"def backward(self)\": \"This is our backward pass from earlier, but save it inside `self.inp.g`\"\n};\n\n\nclass Linear():\n    def __init__(self, w, b):\n        self.w, self.b = w, b\n    \n    def __call__(self, inp):\n        self.inp = inp\n        self.out = inp @ self.w + self.b\n        return self.out\n    \n    def backward(self):\n        self.inp.g = self.out.g @ self.w.t()\n        # Creating a giant outer product just to sum it together is very inefficient. Do it all at once!\n        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n        self.b.g = self.out.g.sum(0)\n\n\nclass MSE():\n    def __call__(self, inp, targ):\n        self.inp = inp\n        self.targ = targ\n        self.out = (inp.squeeze() - targ).pow(2).mean()\n        return self.out\n    \n    def backward(self):\n        self.inp.g = 2. * (self.inp.squeeze(-1) - self.targ).unsqueeze(-1) / self.targ.shape[0]\n\n\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Linear(w1,b1), ReLU(), Linear(w2,b2)]\n        self.loss = MSE()\n    \n    def __call__(self, x, targ):\n        for layer in self.layers:\n            x = layer(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for layer in reversed(self.layers):\n            layer.backward()\n\n\n# Reset our gradients:\nw1.g, b1.g, w2.g, b2.g = [None]*4\n# And define the model\n\nmodel = Model(w1, b1, w2, b2)\n\n\n\n\nCPU times: user 77.4 ms, sys: 26.8 ms, total: 104 ms\nWall time: 13.2 ms\n\n\n\n\n\nCPU times: user 3.62 s, sys: 2.41 s, total: 6.03 s\nWall time: 893 ms\n\n\n\n# Check the gradients align\ntest_close(w2g, w2.g)\ntest_close(b2g, b2.g)\ntest_close(w1g, w1.g)\ntest_close(b1g, b1.g)\ntest_close(ig, x_train.g)",
    "crumbs": [
      "Lesson 1, The forward and backward passes (part 2)"
    ]
  },
  {
    "objectID": "fully_connected.html#refactor-again",
    "href": "fully_connected.html#refactor-again",
    "title": "Lesson 1, The forward and backward passes (part 2)",
    "section": "Refactor again",
    "text": "Refactor again\n\nclass Module():\n    \"Basic class that will impelement .backward() and store the args and outputs from the forward function\"\n    def __call__(self, *args):\n        self.args = args\n        self.out = self.forward(*args)\n        return self.out\n    \n    def forward(self): \n        raise NotImplementedError(\"You need to define the forward funciton still!\")\n    \n    def backward(self):\n        self.bwd(self.out, *self.args)\n\n\nclass ReLU(Module):\n    def forward(self, inp):\n        return inp.clamp_min(0.)-0.5\n    \n    def bwd(self, out, inp):\n        inp.g = (inp&gt;0).float() * out.g\n\n\nclass Linear(Module):\n    def __init__(self, w, b):\n        self.w, self.b = w, b\n    \n    def forward(self, inp):\n        return inp@self.w + self.b\n    \n    def bwd(self, out, inp):\n        inp.g = out.g @ self.w.t()\n        # Creating a giant outer product just to sum it together is very inefficient. Do it all at once!\n        self.w.g = torch.einsum(\"bi,bj-&gt;ij\",inp,out.g)\n        self.b.g = out.g.sum(0)\n\n\nclass MSE(Module):\n    def forward(self, inp, targ):\n        return (inp.squeeze() - targ).pow(2).mean()\n    \n    def bwd(self, out, inp, targ):\n        inp.g = 2. * (inp.squeeze(-1) - targ).unsqueeze(-1) / targ.shape[0]\n\n\nclass Model():\n    def __init__(self, w1, b1, w2, b2):\n        self.layers = [Linear(w1,b1), ReLU(), Linear(w2,b2)]\n        self.loss = MSE()\n    \n    def __call__(self, x, targ):\n        for layer in self.layers:\n            x = layer(x)\n        return self.loss(x, targ)\n    \n    def backward(self):\n        self.loss.backward()\n        for layer in reversed(self.layers):\n            layer.backward()\n\n\nw1.g, b1.g, w2.g, b2.g = [None]*4\nmodel = Model(w1, b1, w2, b2)\n\nCPU times: user 142 ms, sys: 0 ns, total: 142 ms\nWall time: 19.9 ms\n\n\n\n\n\nCPU times: user 234 ms, sys: 157 ms, total: 391 ms\nWall time: 49.7 ms",
    "crumbs": [
      "Lesson 1, The forward and backward passes (part 2)"
    ]
  },
  {
    "objectID": "fully_connected.html#nn.linear-and-nn.module",
    "href": "fully_connected.html#nn.linear-and-nn.module",
    "title": "Lesson 1, The forward and backward passes (part 2)",
    "section": "nn.Linear and nn.Module",
    "text": "nn.Linear and nn.Module\nWe have now implemented both of these, and thus we’re allowed to use them\n\nclass Model(nn.Module):\n    def __init__(self, n_in, nh, n_out):\n        super().__init__()\n        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n        self.loss = mse\n    \n    def __call__(self, x, targ):\n        for layer in self.layers:\n            x = layer(x)\n        return self.loss(x.squeeze(-1), targ)\n\n\nmodel = Model(m, nh, 1)\n\n\n\n\nCPU times: user 129 ms, sys: 2.81 ms, total: 131 ms\nWall time: 19.7 ms\n\n\n\n\n\nCPU times: user 105 ms, sys: 0 ns, total: 105 ms\nWall time: 16 ms",
    "crumbs": [
      "Lesson 1, The forward and backward passes (part 2)"
    ]
  }
]